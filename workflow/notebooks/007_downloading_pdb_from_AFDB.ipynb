{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import shutil\n",
    "import multiprocessing\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file1 = '../../results/reciprocal_best_hit_SingleOrgApproach_TSV/rbh_all_in_one_file.tsv' \n",
    "\n",
    "outputfilepath = '../tmp/TESTING_query_taget_accesion_to_fatcat_list.tsv'\n",
    "\n",
    "first_target_directory = '../tmp/FATCAT_target_pdb_files'\n",
    "\n",
    "destination_dir = '../tmp/FATCAT_pdb_files/'\n",
    "\n",
    "cluster_str_rep_path = '../genome_data_sets/query_proteomes/pdb_files/cluster_structure_representers/'\n",
    "\n",
    "num_cores = 30\n",
    "\n",
    "top_hits = 10\n",
    "\n",
    "prefix = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = snakemake.input.file1 \n",
    "\n",
    "outputfilepath = snakemake.output.tsv_to_match_pdbs_names\n",
    "\n",
    "#here we will download pdbs from AFDB\n",
    "first_target_directory = 'tmp/FATCAT_target_pdb_files/'\n",
    "\n",
    "# destination directory\n",
    "destination_dir = snakemake.params.destination_dir\n",
    "\n",
    "# cluster_structure_representers path\n",
    "cluster_str_rep_path = 'genome_data_sets/query_proteomes/pdb_files/cluster_structure_representers/'\n",
    "\n",
    "#cores\n",
    "num_cores = snakemake.threads\n",
    "\n",
    "#top hits\n",
    "top_hits = snakemake.params.top_hits\n",
    "\n",
    "#prefix added pdbs\n",
    "prefix = snakemake.params.prefix_of_added_pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing and creating destination dir\n",
    "\n",
    "if os.path.exists(destination_dir):\n",
    "    shutil.rmtree(destination_dir)\n",
    "os.makedirs(destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBH results\n",
    "df_rbh = pd.read_csv(file1, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rbh = df_rbh.drop_duplicates(subset=['query_uniprot_accession', 'target_uniprot_accession'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rbh_top_hits = (\n",
    "    df_rbh\n",
    "    .sort_values('evalue')\n",
    "    .groupby('query')\n",
    "    .head(top_hits)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplifiying names of pdb files\n",
    "df_rbh_top_hits['new_simple_name'] =  (\n",
    "\n",
    "    df_rbh_top_hits['query_uniprot_accession']\n",
    "    .str.replace(prefix,'')\n",
    "    .str.split('_', expand=True)[0]\n",
    "    .str.replace('.','')\n",
    "    + '.pdb'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#simplifiying names of pdb files\n",
    "df_rbh_top_hits['new_simple_name_target'] =  (\n",
    "\n",
    "    df_rbh_top_hits['target_uniprot_accession']\n",
    "    .str.replace(prefix,'')\n",
    "    .str.split('_', expand=True)[0]\n",
    "    .str.replace('.','')\n",
    "    + '.pdb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating path to copy query structures and simplify file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "df_rbh_km_tophits_to_move_files = df_rbh_top_hits[['query', 'query_uniprot_accession', 'new_simple_name']].drop_duplicates()\n",
    "#adding path to file\n",
    "df_rbh_km_tophits_to_move_files['path_to_file'] = cluster_str_rep_path + df_rbh_km_tophits_to_move_files['query']\n",
    "#creating the list of tuples\n",
    "files_list = list(zip(df_rbh_km_tophits_to_move_files.path_to_file, df_rbh_km_tophits_to_move_files.new_simple_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(file, destination_dir):\n",
    "    \"\"\"Copy a file to a destination directory.\"\"\"\n",
    "    file_path, new_name = file\n",
    "    shutil.copy(file_path, os.path.join(destination_dir, new_name))\n",
    "\n",
    "\n",
    "def copy_files(files, destination_dir, num_cores=4):\n",
    "    \"\"\"Copy a list of files to a destination directory in parallel.\"\"\"\n",
    "    if os.path.exists(destination_dir):\n",
    "        shutil.rmtree(destination_dir)\n",
    "    os.makedirs(destination_dir)\n",
    "    \n",
    "    with multiprocessing.Pool(num_cores) as pool:\n",
    "        pool.starmap(copy_file, [(file, destination_dir) for file in files])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the files to the destination directory in parallel\n",
    "copy_files(files_list, destination_dir, num_cores= num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading target structures from AFDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading structures from AlphaFold Data Base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 518/68100 [00:09<20:27, 55.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:851\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloading structures from AlphaFold Data Base.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Mapa los enlaces a la función de descarga en paralelo\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m url: download_file(url, dest_dir), links), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(links)):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Cerrar el objeto ThreadPool para liberar recursos\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Definir una función para descargar un archivo desde una URL y guardarla en un directorio\n",
    "def download_file(url, dest_dir):\n",
    "    # Obtener el nombre del archivo a partir de la URL\n",
    "    file_name = os.path.basename(url)\n",
    "    # Construir la ruta completa del archivo de destino\n",
    "    dest_path = os.path.join(dest_dir, file_name)\n",
    "    \n",
    "    # Descargar el archivo y escribirlo en disco\n",
    "    if os.path.exists(dest_path):\n",
    "        #print(f\"Skipping {url} - file already exists in {output_dir}\")\n",
    "        return\n",
    "    with open(dest_path, \"wb\") as file:\n",
    "        response = requests.get(url)\n",
    "        file.write(response.content)\n",
    "        \n",
    "# Lista de enlaces que se van a descargar\n",
    "links = [f'https://alphafold.ebi.ac.uk/files/AF-{UNIPROTaccession}-F1-model_v4.pdb' for UNIPROTaccession in df_rbh_top_hits.target_uniprot_accession.unique()]\n",
    "\n",
    "# Directorio de destino para las descargas\n",
    "dest_dir = first_target_directory\n",
    "\n",
    "# Crear el directorio de destino si no existe\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "    \n",
    "# Crear un objeto ThreadPool con un número de hilos adecuado\n",
    "pool = ThreadPool(num_cores) # aquí se utiliza 4 hilos, pero esto se puede ajustar según el tamaño de su máquina\n",
    "\n",
    "#\n",
    "print('Downloading structures from AlphaFold Data Base.')\n",
    "\n",
    "# Mapa los enlaces a la función de descarga en paralelo\n",
    "for _ in tqdm(pool.imap_unordered(lambda url: download_file(url, dest_dir), links), total=len(links)):\n",
    "    pass\n",
    "\n",
    "# Cerrar el objeto ThreadPool para liberar recursos\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print('Download Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move files and delete directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_wo_rmDir(files, destination_dir, num_cores=4):\n",
    "    \"\"\"Copy a list of files to a destination directory in parallel.\"\"\"\n",
    "    \n",
    "    with multiprocessing.Pool(num_cores) as pool:\n",
    "        pool.starmap(copy_file, [(file, destination_dir) for file in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(first_target_directory)\n",
    "\n",
    "\n",
    "target_file_list = [(first_target_directory + file,  file.split('-')[1] + '.pdb') for file in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the files to the destination directory in parallel\n",
    "copy_files_wo_rmDir(target_file_list, destination_dir, num_cores= num_cores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 39] Directory not empty: '../tmp/FATCAT_target_pdb_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(first_target_directory):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_target_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/shutil.py:722\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    720\u001b[0m         os\u001b[38;5;241m.\u001b[39mrmdir(path)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m         \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;66;03m# symlinks to directories are forbidden, see bug #1669\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/shutil.py:720\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    718\u001b[0m _rmtree_safe_fd(fd, path, onerror)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     onerror(os\u001b[38;5;241m.\u001b[39mrmdir, path, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 39] Directory not empty: '../tmp/FATCAT_target_pdb_files'"
     ]
    }
   ],
   "source": [
    "if os.path.exists(first_target_directory):\n",
    "    shutil.rmtree(first_target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating comparison file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_file_OLD_NEW_names(df, outputfilepath):\n",
    "    \n",
    "    df = df.sort_values('query_uniprot_accession')\n",
    "\n",
    "    df[['query','target', 'query_uniprot_accession', 'target_uniprot_accession']].to_csv(outputfilepath, sep='\\t', index=False)\n",
    "    \n",
    "    \n",
    "creating_file_OLD_NEW_names(df_rbh_top_hits, outputfilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
